{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Text2SQL Agent Walkthrough\n",
    "\n",
    "This notebook will walk users through setting up a generic Text2SQL Agent and run it against the BirdSQL - Mini Dev Dataset (https://github.com/bird-bench/mini_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure the latest version of boto3 is shown below\n",
    "\n",
    "##### If not then run through setup_environment.ipynb in the 0-Notebook-environment/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3==1.36.24\n",
      "pandas==1.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep -E \"boto3|pandas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in environment variables to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve import path\n",
    "%store -r IMPORTS_PATH\n",
    "\n",
    "# Retrieve account info\n",
    "%store -r account_id\n",
    "%store -r region\n",
    "\n",
    "# Retrieve model lists\n",
    "%store -r agent_foundation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'BASE_BUCKET_NAME' (str)\n",
      "Stored 'ATHENA_RESULTS_BUCKET_NAME' (str)\n",
      "Stored 'BASE_DIR' (str)\n",
      "Stored 'DATABASE_NAME' (str)\n"
     ]
    }
   ],
   "source": [
    "BASE_BUCKET_NAME = 'text2sql-agent'\n",
    "ATHENA_RESULTS_BUCKET_NAME = 'text2sql-athena-results'\n",
    "BASE_DIR = 'dev_databases'\n",
    "DATABASE_NAME = 'california_schools'\n",
    "\n",
    "# Store the variables\n",
    "\n",
    "%store BASE_BUCKET_NAME\n",
    "%store ATHENA_RESULTS_BUCKET_NAME\n",
    "%store BASE_DIR\n",
    "%store DATABASE_NAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve BirdSQL - Mini Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 0: cd: 2-Sample-text2sql-agent: No such file or directory\n",
      "--2025-02-19 22:10:25--  https://bird-bench.oss-cn-beijing.aliyuncs.com/dev.zip\n",
      "Resolving bird-bench.oss-cn-beijing.aliyuncs.com (bird-bench.oss-cn-beijing.aliyuncs.com)... 8.141.181.247\n",
      "connected. to bird-bench.oss-cn-beijing.aliyuncs.com (bird-bench.oss-cn-beijing.aliyuncs.com)|8.141.181.247|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 346207293 (330M) [application/zip]\n",
      "Saving to: ‘dev.zip’\n",
      "\n",
      "100%[======================================>] 346,207,293 16.0MB/s   in 22s    \n",
      "\n",
      "2025-02-19 22:10:48 (14.9 MB/s) - ‘dev.zip’ saved [346207293/346207293]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download .zip file to local directory\n",
    "\n",
    "!cd 2-Sample-text2sql-agent\n",
    "!wget https://bird-bench.oss-cn-beijing.aliyuncs.com/dev.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load data into Amazon S3 - logic for this needs to be coded\n",
    "# Unzip dev.zip and load 1 of the .sqlite files into S3 if possible/Redshift Serverless\n",
    "# Keep in mind that Text2SQL Agent must be able to access data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Necessary services to run Text2SQL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the Text2SQL agent, we will need to setup the Athena databases to make SQL queries against. The following script will:\n",
    "1. Unzip the downloaded folder\n",
    "2. Create S3 buckets\n",
    "3. Convert .sqlite files into individual .parquet files for each table\n",
    "4. Upload to the database s3 bucket\n",
    "5. Set up appropriate Athena permissions\n",
    "6. Create databases in Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "%run data_prep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the .sqlite files into .parquet for Athena queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1) unzip the dev.zip and the databases zip file within the dev folder to get the .sqlite files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: unzipped_dev\n",
      "Unzipped dev.zip to unzipped_dev\n",
      "[PosixPath('unzipped_dev/dev_20240627')]\n",
      "Found directory: unzipped_dev/dev_20240627\n",
      "unzipped_dev/dev_20240627/dev_databases.zip\n",
      "Unzipped dev_databases.zip in /home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "def create_and_unzip(first_zip, new_directory, second_zip):\n",
    "    notebook_dir = Path.cwd()\n",
    "    new_dir = Path(new_directory)\n",
    "    new_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created directory: {new_dir}\")\n",
    "    \n",
    "    # Unzip first file into new directory\n",
    "    with zipfile.ZipFile(first_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(new_directory)\n",
    "    print(f\"Unzipped {first_zip} to {new_directory}\")\n",
    "    \n",
    "    # Create full path to subfolder\n",
    "    subdirs = [d for d in new_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "    if subdirs:\n",
    "        auto_found_dir = subdirs[0]\n",
    "        print(f\"Found directory: {auto_found_dir}\")\n",
    "\n",
    "        second_zip_path = auto_found_dir / second_zip\n",
    "        print(second_zip_path)\n",
    "        if second_zip_path.exists():\n",
    "            with zipfile.ZipFile(second_zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(notebook_dir)\n",
    "            print(f\"Unzipped {second_zip} in {notebook_dir}\")\n",
    "        else:\n",
    "            print(f\"Could not find {second_zip} in {auto_found_dir}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No directories found in {new_dir}\")\n",
    "\n",
    "\n",
    "create_and_unzip(\n",
    "    first_zip='dev.zip',\n",
    "    new_directory='unzipped_dev',\n",
    "    second_zip='dev_databases.zip'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2) Go through all the subfolders, convert sqlite files into parquet and upload to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket: text2sql-agent-761018876800-8a4c4e\n",
      "Applied bucket policy\n",
      "Main bucket setup complete\n"
     ]
    }
   ],
   "source": [
    "#Create and setup s3 bucket\n",
    "\n",
    "def get_account_id():\n",
    "    \"\"\"Get the current AWS account ID\"\"\"\n",
    "    sts_client = boto3.client('sts')\n",
    "    return sts_client.get_caller_identity()['Account']\n",
    "\n",
    "def create_unique_bucket_name(base_name):\n",
    "    \"\"\"Create a unique bucket name using account ID and a random suffix\"\"\"\n",
    "    account_id = get_account_id()\n",
    "    random_suffix = uuid.uuid4().hex[:6]  # 6 character random string\n",
    "    return f\"{base_name}-{account_id}-{random_suffix}\"\n",
    "\n",
    "def create_s3_bucket_with_permissions(base_bucket_name, region='us-west-2'):\n",
    "    \"\"\"\n",
    "    Create S3 bucket with a unique name and appropriate permissions\n",
    "    \"\"\"\n",
    "    bucket_name = create_unique_bucket_name(base_bucket_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    try:\n",
    "        # Create bucket\n",
    "        s3_client.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "        )\n",
    "        print(f\"Created bucket: {bucket_name}\")\n",
    "\n",
    "        # Create bucket policy\n",
    "        bucket_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Sid\": \"AllowAthenaAccess\",\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                        \"Service\": \"athena.amazonaws.com\"\n",
    "                    },\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetBucketLocation\",\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\",\n",
    "                        \"s3:PutObject\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        f\"arn:aws:s3:::{bucket_name}\",\n",
    "                        f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"Sid\": \"AllowCurrentUserReadWrite\",\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                        \"AWS\": boto3.client('sts').get_caller_identity()['Arn']\n",
    "                    },\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:PutObject\",\n",
    "                        \"s3:DeleteObject\",\n",
    "                        \"s3:ListBucket\",\n",
    "                        \"s3:GetBucketLocation\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        f\"arn:aws:s3:::{bucket_name}\",\n",
    "                        f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Convert policy to JSON string\n",
    "        bucket_policy = json.dumps(bucket_policy)\n",
    "\n",
    "        # Set bucket policy\n",
    "        s3_client.put_bucket_policy(\n",
    "            Bucket=bucket_name,\n",
    "            Policy=bucket_policy\n",
    "        )\n",
    "        print(\"Applied bucket policy\")\n",
    "\n",
    "        return bucket_name\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Configuration\n",
    "BASE_BUCKET_NAME = 'text2sql-agent'\n",
    "REGION = 'us-west-2'\n",
    "\n",
    "# Create main bucket\n",
    "main_bucket = create_s3_bucket_with_permissions(BASE_BUCKET_NAME, REGION)\n",
    "if main_bucket:\n",
    "    print(\"Main bucket setup complete\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to set up main bucket\")\n",
    "\n",
    "\n",
    "#Create athena results bucket\n",
    "athena_results_bucket_name = 'text2sql-athena-results'\n",
    "\n",
    "# Create results bucket\n",
    "athena_results_bucket = create_s3_bucket_with_permissions(athena_results_bucket_name, REGION)\n",
    "if main_bucket:\n",
    "    print(\"Main bucket setup complete\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to set up main bucket\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text2sql-agent-761018876800-8a4c4e'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_bucket, athena_results_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing database: toxicology\n",
      "SQLite file: dev_databases/toxicology/toxicology.sqlite\n",
      "\n",
      "Processing table: atom\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/toxicology/atom.parquet\n",
      "\n",
      "Processing table: bond\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/toxicology/bond.parquet\n",
      "\n",
      "Processing table: connected\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/toxicology/connected.parquet\n",
      "\n",
      "Processing table: molecule\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/toxicology/molecule.parquet\n",
      "\n",
      "Processing database: student_club\n",
      "SQLite file: dev_databases/student_club/student_club.sqlite\n",
      "\n",
      "Processing table: event\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/event.parquet\n",
      "\n",
      "Processing table: major\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/major.parquet\n",
      "\n",
      "Processing table: zip_code\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/zip_code.parquet\n",
      "\n",
      "Processing table: attendance\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/attendance.parquet\n",
      "\n",
      "Processing table: budget\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/budget.parquet\n",
      "\n",
      "Processing table: expense\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/expense.parquet\n",
      "\n",
      "Processing table: income\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/income.parquet\n",
      "\n",
      "Processing table: member\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/student_club/member.parquet\n",
      "\n",
      "Processing database: debit_card_specializing\n",
      "SQLite file: dev_databases/debit_card_specializing/debit_card_specializing.sqlite\n",
      "\n",
      "Processing table: customers\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/debit_card_specializing/customers.parquet\n",
      "\n",
      "Processing table: gasstations\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/debit_card_specializing/gasstations.parquet\n",
      "\n",
      "Processing table: products\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/debit_card_specializing/products.parquet\n",
      "\n",
      "Processing table: transactions_1k\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/debit_card_specializing/transactions_1k.parquet\n",
      "\n",
      "Processing table: sqlite_sequence\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/debit_card_specializing/sqlite_sequence.parquet\n",
      "\n",
      "Processing table: yearmonth\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/debit_card_specializing/yearmonth.parquet\n",
      "\n",
      "Processing database: thrombosis_prediction\n",
      "SQLite file: dev_databases/thrombosis_prediction/thrombosis_prediction.sqlite\n",
      "\n",
      "Processing table: Examination\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/thrombosis_prediction/Examination.parquet\n",
      "\n",
      "Processing table: Patient\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/thrombosis_prediction/Patient.parquet\n",
      "\n",
      "Processing table: Laboratory\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/thrombosis_prediction/Laboratory.parquet\n",
      "\n",
      "Processing database: card_games\n",
      "SQLite file: dev_databases/card_games/card_games.sqlite\n",
      "\n",
      "Processing table: sqlite_sequence\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/card_games/sqlite_sequence.parquet\n",
      "\n",
      "Processing table: cards\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/card_games/cards.parquet\n",
      "\n",
      "Processing table: foreign_data\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/card_games/foreign_data.parquet\n",
      "\n",
      "Processing table: legalities\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/card_games/legalities.parquet\n",
      "\n",
      "Processing table: sets\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/card_games/sets.parquet\n",
      "\n",
      "Processing table: set_translations\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/card_games/set_translations.parquet\n",
      "\n",
      "Processing table: rulings\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/card_games/rulings.parquet\n",
      "\n",
      "Processing database: california_schools\n",
      "SQLite file: dev_databases/california_schools/california_schools.sqlite\n",
      "\n",
      "Processing table: frpm\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/california_schools/frpm.parquet\n",
      "\n",
      "Processing table: satscores\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/california_schools/satscores.parquet\n",
      "\n",
      "Processing table: schools\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/california_schools/schools.parquet\n",
      "\n",
      "Processing database: european_football_2\n",
      "SQLite file: dev_databases/european_football_2/european_football_2.sqlite\n",
      "\n",
      "Processing table: sqlite_sequence\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/sqlite_sequence.parquet\n",
      "\n",
      "Processing table: Player_Attributes\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/Player_Attributes.parquet\n",
      "\n",
      "Processing table: Player\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/Player.parquet\n",
      "\n",
      "Processing table: League\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/League.parquet\n",
      "\n",
      "Processing table: Country\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/Country.parquet\n",
      "\n",
      "Processing table: Team\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/Team.parquet\n",
      "\n",
      "Processing table: Team_Attributes\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/Team_Attributes.parquet\n",
      "\n",
      "Processing table: Match\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/european_football_2/Match.parquet\n",
      "\n",
      "Processing database: financial\n",
      "SQLite file: dev_databases/financial/financial.sqlite\n",
      "\n",
      "Processing table: account\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/account.parquet\n",
      "\n",
      "Processing table: card\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/card.parquet\n",
      "\n",
      "Processing table: client\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/client.parquet\n",
      "\n",
      "Processing table: disp\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/disp.parquet\n",
      "\n",
      "Processing table: district\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/district.parquet\n",
      "\n",
      "Processing table: loan\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/loan.parquet\n",
      "\n",
      "Processing table: order\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/order.parquet\n",
      "\n",
      "Processing table: trans\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/financial/trans.parquet\n",
      "\n",
      "Processing database: superhero\n",
      "SQLite file: dev_databases/superhero/superhero.sqlite\n",
      "\n",
      "Processing table: alignment\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/alignment.parquet\n",
      "\n",
      "Processing table: attribute\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/attribute.parquet\n",
      "\n",
      "Processing table: colour\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/colour.parquet\n",
      "\n",
      "Processing table: gender\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/gender.parquet\n",
      "\n",
      "Processing table: publisher\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/publisher.parquet\n",
      "\n",
      "Processing table: race\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/race.parquet\n",
      "\n",
      "Processing table: superhero\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/superhero.parquet\n",
      "\n",
      "Processing table: hero_attribute\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/hero_attribute.parquet\n",
      "\n",
      "Processing table: superpower\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/superpower.parquet\n",
      "\n",
      "Processing table: hero_power\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/superhero/hero_power.parquet\n",
      "\n",
      "Processing database: formula_1\n",
      "SQLite file: dev_databases/formula_1/formula_1.sqlite\n",
      "\n",
      "Processing table: sqlite_sequence\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/sqlite_sequence.parquet\n",
      "\n",
      "Processing table: circuits\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/circuits.parquet\n",
      "\n",
      "Processing table: constructors\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/constructors.parquet\n",
      "\n",
      "Processing table: drivers\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/drivers.parquet\n",
      "\n",
      "Processing table: seasons\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/seasons.parquet\n",
      "\n",
      "Processing table: races\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/races.parquet\n",
      "\n",
      "Processing table: constructorResults\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/constructorResults.parquet\n",
      "\n",
      "Processing table: constructorStandings\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/constructorStandings.parquet\n",
      "\n",
      "Processing table: driverStandings\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/driverStandings.parquet\n",
      "\n",
      "Processing table: lapTimes\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/lapTimes.parquet\n",
      "\n",
      "Processing table: pitStops\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/pitStops.parquet\n",
      "\n",
      "Processing table: qualifying\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/qualifying.parquet\n",
      "\n",
      "Processing table: status\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/status.parquet\n",
      "\n",
      "Processing table: results\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/formula_1/results.parquet\n",
      "\n",
      "Processing database: codebase_community\n",
      "SQLite file: dev_databases/codebase_community/codebase_community.sqlite\n",
      "\n",
      "Processing table: badges\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/badges.parquet\n",
      "\n",
      "Processing table: comments\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/comments.parquet\n",
      "\n",
      "Processing table: postHistory\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/postHistory.parquet\n",
      "\n",
      "Processing table: postLinks\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/postLinks.parquet\n",
      "\n",
      "Processing table: posts\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/posts.parquet\n",
      "\n",
      "Processing table: tags\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/tags.parquet\n",
      "\n",
      "Processing table: users\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/users.parquet\n",
      "\n",
      "Processing table: votes\n",
      "Uploaded to s3://text2sql-agent-761018876800-8a4c4e/codebase_community/votes.parquet\n",
      "Upload complete\n"
     ]
    }
   ],
   "source": [
    "# Now go through all the subfolders to find .sqlite files to convert to .parquet files and upload to s3\n",
    "\n",
    "def process_database_and_upload(database_folder, bucket_name):\n",
    "    \"\"\"\n",
    "    Process SQLite database, convert to parquet, and upload to S3\n",
    "    All tables from same database go to same folder\n",
    "    \"\"\"\n",
    "    folder_path = Path(database_folder)\n",
    "    database_name = folder_path.name\n",
    "    \n",
    "    # Find SQLite file\n",
    "    sqlite_files = list(folder_path.glob('*.db')) + list(folder_path.glob('*.sqlite'))\n",
    "    if not sqlite_files:\n",
    "        print(f\"No SQLite file found in {database_folder}\")\n",
    "        return\n",
    "    \n",
    "    sqlite_file = sqlite_files[0]\n",
    "    print(f\"\\nProcessing database: {database_name}\")\n",
    "    print(f\"SQLite file: {sqlite_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Connect to SQLite database\n",
    "        conn = sqlite3.connect(sqlite_file)\n",
    "        \n",
    "        # Get list of all tables\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        \n",
    "        # Create S3 client\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Process each table\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            print(f\"\\nProcessing table: {table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read table into DataFrame\n",
    "                df = pd.read_sql_query(f'SELECT * FROM \"{table_name}\"', conn)\n",
    "                \n",
    "                # Create local parquet file\n",
    "                parquet_filename = f\"{table_name}.parquet\"\n",
    "                local_parquet_path = folder_path / parquet_filename\n",
    "                df.to_parquet(local_parquet_path, index=False)\n",
    "                \n",
    "                # Upload to S3 (all tables in same database folder)\n",
    "                s3_key = f\"{database_name}/{parquet_filename}\"\n",
    "                s3_client.upload_file(\n",
    "                    str(local_parquet_path),\n",
    "                    bucket_name,\n",
    "                    s3_key\n",
    "                )\n",
    "                print(f\"Uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "                \n",
    "                # Clean up local parquet file\n",
    "                os.remove(local_parquet_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing table {table_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing database {database_name}: {str(e)}\")\n",
    "        return\n",
    "\n",
    "\n",
    "BASE_DIR = 'dev_databases'\n",
    "BUCKET_NAME = main_bucket\n",
    "\n",
    "# Process each database folder\n",
    "base_path = Path(BASE_DIR)\n",
    "for database_folder in base_path.iterdir():\n",
    "    if database_folder.is_dir():\n",
    "        process_database_and_upload(database_folder, BUCKET_NAME)\n",
    "\n",
    "print(\"Upload complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3) Create external tables for each table in Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing policy\n",
      "Created policy: arn:aws:iam::761018876800:policy/AthenaQueryPermissions\n",
      "Attached policy to role: biomakeragent-AgentBuildNest-SageMakerExecutionRole-kvD8zSoPKC2k\n",
      "Successfully added Athena permissions with specific bucket access\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update current role to include permissions for athena \n",
    "def add_athena_permissions(data_bucket, results_bucket):\n",
    "    iam = boto3.client('iam')\n",
    "    sts = boto3.client('sts')\n",
    "\n",
    "    # Get current user/role info\n",
    "    caller_identity = sts.get_caller_identity()\n",
    "    current_user_arn = caller_identity['Arn']\n",
    "    account_id = caller_identity['Account']\n",
    "    region = boto3.session.Session().region_name\n",
    "\n",
    "    # Define the policy\n",
    "    athena_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"athena:*\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:athena:*:{account_id}:capacity-reservation/*\",\n",
    "                    f\"arn:aws:athena:*:{account_id}:workgroup/primary\",\n",
    "                    f\"arn:aws:athena:*:{account_id}:datacatalog/*\"\n",
    "                ]\n",
    "            },\n",
    "\n",
    "            {\n",
    "                # Data bucket permissions (read-only)\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetBucketLocation\",\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{data_bucket}\",\n",
    "                    f\"arn:aws:s3:::{data_bucket}/*\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                # Results bucket permissions (read and write)\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetBucketLocation\",\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\",\n",
    "                    \"s3:PutObject\",\n",
    "                    \"s3:ListMultipartUploadParts\",\n",
    "                    \"s3:AbortMultipartUpload\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{results_bucket}\",\n",
    "                    f\"arn:aws:s3:::{results_bucket}/*\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"glue:CreateDatabase\",\n",
    "                    \"glue:DeleteDatabase\",\n",
    "                    \"glue:GetCatalog\",\n",
    "                    \"glue:GetCatalogs\",\n",
    "                    \"glue:GetDatabase\",\n",
    "                    \"glue:GetDatabases\",\n",
    "                    \"glue:UpdateDatabase\",\n",
    "                    \"glue:CreateTable\",\n",
    "                    \"glue:DeleteTable\",\n",
    "                    \"glue:BatchDeleteTable\",\n",
    "                    \"glue:UpdateTable\",\n",
    "                    \"glue:GetTable\",\n",
    "                    \"glue:GetTables\",\n",
    "                    \"glue:BatchCreatePartition\",\n",
    "                    \"glue:CreatePartition\",\n",
    "                    \"glue:DeletePartition\",\n",
    "                    \"glue:BatchDeletePartition\",\n",
    "                    \"glue:UpdatePartition\",\n",
    "                    \"glue:GetPartition\",\n",
    "                    \"glue:GetPartitions\",\n",
    "                    \"glue:BatchGetPartition\",\n",
    "                    \"glue:StartColumnStatisticsTaskRun\",\n",
    "                    \"glue:GetColumnStatisticsTaskRun\",\n",
    "                    \"glue:GetColumnStatisticsTaskRuns\",\n",
    "                    \"glue:GetCatalogImportStatus\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:glue:*:{account_id}:catalog\",\n",
    "                    f\"arn:aws:glue:*:{account_id}:database/*\",\n",
    "                    f\"arn:aws:glue:*:{account_id}:table/*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Delete existing policy if it exists\n",
    "        try:\n",
    "            existing_policies = iam.list_policies(Scope='Local')['Policies']\n",
    "            for policy in existing_policies:\n",
    "                if policy['PolicyName'] == 'AthenaQueryPermissions':\n",
    "                    iam.delete_policy(PolicyArn=policy['Arn'])\n",
    "                    print(\"Deleted existing policy\")\n",
    "        except Exception as e:\n",
    "            print(f\"Note: {e}\")\n",
    "\n",
    "        # Create the policy\n",
    "        response = iam.create_policy(\n",
    "            PolicyName='AthenaQueryPermissions',\n",
    "            PolicyDocument=json.dumps(athena_policy)\n",
    "        )\n",
    "        policy_arn = response['Policy']['Arn']\n",
    "        print(f\"Created policy: {policy_arn}\")\n",
    "\n",
    "        # Attach the policy to the current user/role\n",
    "        if ':user/' in current_user_arn:\n",
    "            username = current_user_arn.split('/')[-1]\n",
    "            try:\n",
    "                iam.detach_user_policy(\n",
    "                    UserName=username,\n",
    "                    PolicyArn=policy_arn\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "            iam.attach_user_policy(\n",
    "                UserName=username,\n",
    "                PolicyArn=policy_arn\n",
    "            )\n",
    "            print(f\"Attached policy to user: {username}\")\n",
    "        elif ':assumed-role/' in current_user_arn:\n",
    "            role_name = current_user_arn.split('/')[-2]\n",
    "            try:\n",
    "                iam.detach_role_policy(\n",
    "                    RoleName=role_name,\n",
    "                    PolicyArn=policy_arn\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "            iam.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn=policy_arn\n",
    "            )\n",
    "            print(f\"Attached policy to role: {role_name}\")\n",
    "\n",
    "        print(\"Successfully added Athena permissions with specific bucket access\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding permissions: {e}\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "data_bucket = main_bucket      # Bucket with parquet files\n",
    "results_bucket = athena_results_bucket # Bucket for Athena query results\n",
    "\n",
    "add_athena_permissions(data_bucket, results_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set Athena query result location to: s3://text2sql-athena-results-761018876800-fc255e/athena-results/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update the result bucket location for workgroup primary\n",
    "\n",
    "def set_athena_result_location(result_bucket):\n",
    "    \"\"\"\n",
    "    Set the query result location for Athena's primary workgroup\n",
    "    \"\"\"\n",
    "    try:\n",
    "        athena_client = boto3.client('athena')\n",
    "        \n",
    "        # Format the S3 path\n",
    "        s3_output_location = f's3://{result_bucket}/athena-results/'\n",
    "        \n",
    "        # Update primary workgroup configuration\n",
    "        response = athena_client.update_work_group(\n",
    "            WorkGroup='primary',\n",
    "            ConfigurationUpdates={\n",
    "                'ResultConfigurationUpdates': {\n",
    "                    'OutputLocation': s3_output_location\n",
    "                },\n",
    "                'EnforceWorkGroupConfiguration': True\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully set Athena query result location to: {s3_output_location}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting result location: {e}\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "result_bucket = athena_results_bucket # Replace with your bucket name\n",
    "set_athena_result_location(result_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing database: california_schools\n",
      "Created database: california_schools\n",
      "\n",
      "Generating table: california_schools.frpm\n",
      "Created table: california_schools.frpm\n",
      "\n",
      "Generating table: california_schools.satscores\n",
      "Created table: california_schools.satscores\n",
      "\n",
      "Generating table: california_schools.schools\n",
      "Created table: california_schools.schools\n",
      "\n",
      "Processing database: card_games\n",
      "Created database: card_games\n",
      "\n",
      "Generating table: card_games.cards\n",
      "Created table: card_games.cards\n",
      "\n",
      "Generating table: card_games.foreign_data\n",
      "Created table: card_games.foreign_data\n",
      "\n",
      "Generating table: card_games.legalities\n",
      "Created table: card_games.legalities\n",
      "\n",
      "Generating table: card_games.rulings\n",
      "Created table: card_games.rulings\n",
      "\n",
      "Generating table: card_games.set_translations\n",
      "Created table: card_games.set_translations\n",
      "\n",
      "Generating table: card_games.sets\n",
      "Created table: card_games.sets\n",
      "\n",
      "Generating table: card_games.sqlite_sequence\n",
      "Created table: card_games.sqlite_sequence\n",
      "\n",
      "Processing database: codebase_community\n",
      "Created database: codebase_community\n",
      "\n",
      "Generating table: codebase_community.badges\n",
      "Created table: codebase_community.badges\n",
      "\n",
      "Generating table: codebase_community.comments\n",
      "Created table: codebase_community.comments\n",
      "\n",
      "Generating table: codebase_community.postHistory\n",
      "Created table: codebase_community.postHistory\n",
      "\n",
      "Generating table: codebase_community.postLinks\n",
      "Created table: codebase_community.postLinks\n",
      "\n",
      "Generating table: codebase_community.posts\n",
      "Created table: codebase_community.posts\n",
      "\n",
      "Generating table: codebase_community.tags\n",
      "Created table: codebase_community.tags\n",
      "\n",
      "Generating table: codebase_community.users\n",
      "Created table: codebase_community.users\n",
      "\n",
      "Generating table: codebase_community.votes\n",
      "Created table: codebase_community.votes\n",
      "\n",
      "Processing database: debit_card_specializing\n",
      "Created database: debit_card_specializing\n",
      "\n",
      "Generating table: debit_card_specializing.customers\n",
      "Created table: debit_card_specializing.customers\n",
      "\n",
      "Generating table: debit_card_specializing.gasstations\n",
      "Created table: debit_card_specializing.gasstations\n",
      "\n",
      "Generating table: debit_card_specializing.products\n",
      "Created table: debit_card_specializing.products\n",
      "\n",
      "Generating table: debit_card_specializing.sqlite_sequence\n",
      "Created table: debit_card_specializing.sqlite_sequence\n",
      "\n",
      "Generating table: debit_card_specializing.transactions_1k\n",
      "Created table: debit_card_specializing.transactions_1k\n",
      "\n",
      "Generating table: debit_card_specializing.yearmonth\n",
      "Created table: debit_card_specializing.yearmonth\n",
      "\n",
      "Processing database: european_football_2\n",
      "Created database: european_football_2\n",
      "\n",
      "Generating table: european_football_2.Country\n",
      "Created table: european_football_2.Country\n",
      "\n",
      "Generating table: european_football_2.League\n",
      "Created table: european_football_2.League\n",
      "\n",
      "Generating table: european_football_2.Match\n",
      "Created table: european_football_2.Match\n",
      "\n",
      "Generating table: european_football_2.Player\n",
      "Created table: european_football_2.Player\n",
      "\n",
      "Generating table: european_football_2.Player_Attributes\n",
      "Created table: european_football_2.Player_Attributes\n",
      "\n",
      "Generating table: european_football_2.Team\n",
      "Created table: european_football_2.Team\n",
      "\n",
      "Generating table: european_football_2.Team_Attributes\n",
      "Created table: european_football_2.Team_Attributes\n",
      "\n",
      "Generating table: european_football_2.sqlite_sequence\n",
      "Created table: european_football_2.sqlite_sequence\n",
      "\n",
      "Processing database: financial\n",
      "Created database: financial\n",
      "\n",
      "Generating table: financial.account\n",
      "Created table: financial.account\n",
      "\n",
      "Generating table: financial.card\n",
      "Created table: financial.card\n",
      "\n",
      "Generating table: financial.client\n",
      "Created table: financial.client\n",
      "\n",
      "Generating table: financial.disp\n",
      "Created table: financial.disp\n",
      "\n",
      "Generating table: financial.district\n",
      "Created table: financial.district\n",
      "\n",
      "Generating table: financial.loan\n",
      "Created table: financial.loan\n",
      "\n",
      "Generating table: financial.order\n",
      "Created table: financial.order\n",
      "\n",
      "Generating table: financial.trans\n",
      "Created table: financial.trans\n",
      "\n",
      "Processing database: formula_1\n",
      "Created database: formula_1\n",
      "\n",
      "Generating table: formula_1.circuits\n",
      "Created table: formula_1.circuits\n",
      "\n",
      "Generating table: formula_1.constructorResults\n",
      "Created table: formula_1.constructorResults\n",
      "\n",
      "Generating table: formula_1.constructorStandings\n",
      "Created table: formula_1.constructorStandings\n",
      "\n",
      "Generating table: formula_1.constructors\n",
      "Created table: formula_1.constructors\n",
      "\n",
      "Generating table: formula_1.driverStandings\n",
      "Created table: formula_1.driverStandings\n",
      "\n",
      "Generating table: formula_1.drivers\n",
      "Created table: formula_1.drivers\n",
      "\n",
      "Generating table: formula_1.lapTimes\n",
      "Created table: formula_1.lapTimes\n",
      "\n",
      "Generating table: formula_1.pitStops\n",
      "Created table: formula_1.pitStops\n",
      "\n",
      "Generating table: formula_1.qualifying\n",
      "Created table: formula_1.qualifying\n",
      "\n",
      "Generating table: formula_1.races\n",
      "Created table: formula_1.races\n",
      "\n",
      "Generating table: formula_1.results\n",
      "Created table: formula_1.results\n",
      "\n",
      "Generating table: formula_1.seasons\n",
      "Created table: formula_1.seasons\n",
      "\n",
      "Generating table: formula_1.sqlite_sequence\n",
      "Created table: formula_1.sqlite_sequence\n",
      "\n",
      "Generating table: formula_1.status\n",
      "Created table: formula_1.status\n",
      "\n",
      "Processing database: student_club\n",
      "Created database: student_club\n",
      "\n",
      "Generating table: student_club.attendance\n",
      "Created table: student_club.attendance\n",
      "\n",
      "Generating table: student_club.budget\n",
      "Created table: student_club.budget\n",
      "\n",
      "Generating table: student_club.event\n",
      "Created table: student_club.event\n",
      "\n",
      "Generating table: student_club.expense\n",
      "Created table: student_club.expense\n",
      "\n",
      "Generating table: student_club.income\n",
      "Created table: student_club.income\n",
      "\n",
      "Generating table: student_club.major\n",
      "Created table: student_club.major\n",
      "\n",
      "Generating table: student_club.member\n",
      "Created table: student_club.member\n",
      "\n",
      "Generating table: student_club.zip_code\n",
      "Created table: student_club.zip_code\n",
      "\n",
      "Processing database: superhero\n",
      "Created database: superhero\n",
      "\n",
      "Generating table: superhero.alignment\n",
      "Created table: superhero.alignment\n",
      "\n",
      "Generating table: superhero.attribute\n",
      "Created table: superhero.attribute\n",
      "\n",
      "Generating table: superhero.colour\n",
      "Created table: superhero.colour\n",
      "\n",
      "Generating table: superhero.gender\n",
      "Created table: superhero.gender\n",
      "\n",
      "Generating table: superhero.hero_attribute\n",
      "Created table: superhero.hero_attribute\n",
      "\n",
      "Generating table: superhero.hero_power\n",
      "Created table: superhero.hero_power\n",
      "\n",
      "Generating table: superhero.publisher\n",
      "Created table: superhero.publisher\n",
      "\n",
      "Generating table: superhero.race\n",
      "Created table: superhero.race\n",
      "\n",
      "Generating table: superhero.superhero\n",
      "Created table: superhero.superhero\n",
      "\n",
      "Generating table: superhero.superpower\n",
      "Created table: superhero.superpower\n",
      "\n",
      "Processing database: thrombosis_prediction\n",
      "Created database: thrombosis_prediction\n",
      "\n",
      "Generating table: thrombosis_prediction.Examination\n",
      "Created table: thrombosis_prediction.Examination\n",
      "\n",
      "Generating table: thrombosis_prediction.Laboratory\n",
      "Created table: thrombosis_prediction.Laboratory\n",
      "\n",
      "Generating table: thrombosis_prediction.Patient\n",
      "Created table: thrombosis_prediction.Patient\n",
      "\n",
      "Processing database: toxicology\n",
      "Created database: toxicology\n",
      "\n",
      "Generating table: toxicology.atom\n",
      "Created table: toxicology.atom\n",
      "\n",
      "Generating table: toxicology.bond\n",
      "Created table: toxicology.bond\n",
      "\n",
      "Generating table: toxicology.connected\n",
      "Created table: toxicology.connected\n",
      "\n",
      "Generating table: toxicology.molecule\n",
      "Created table: toxicology.molecule\n",
      "\n",
      "Completed creating all databases and tables in Athena!\n"
     ]
    }
   ],
   "source": [
    "# loop through all the folders in s3 bucket and create the database and tables in athena\n",
    "\n",
    "import time\n",
    "\n",
    "def list_s3_folders_and_files(bucket_name):\n",
    "    \"\"\"Get all database folders and their parquet files\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # Get all objects in bucket\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        database_tables = {}\n",
    "        \n",
    "        for page in paginator.paginate(Bucket=bucket_name):\n",
    "            if 'Contents' not in page:\n",
    "                continue\n",
    "                \n",
    "            for obj in page['Contents']:\n",
    "                # Split the key into parts\n",
    "                parts = obj['Key'].split('/')\n",
    "                \n",
    "                # Check if it's a parquet file\n",
    "                if len(parts) >= 2 and parts[-1].endswith('.parquet'):\n",
    "                    database_name = parts[0]\n",
    "                    table_name = parts[-1].replace('.parquet', '')\n",
    "                    \n",
    "                    if database_name not in database_tables:\n",
    "                        database_tables[database_name] = []\n",
    "                    database_tables[database_name].append(table_name)\n",
    "        \n",
    "        return database_tables\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"Error listing S3 contents: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_and_create_table(results_bucket_name, parquet_bucket_name, database_name, table_name):\n",
    "    \"\"\"Generate and create a single table\"\"\"\n",
    "    try:\n",
    "        # Generate DDL\n",
    "        s3_path = f's3://{parquet_bucket_name}/{database_name}/{table_name}.parquet'\n",
    "        df = pd.read_parquet(s3_path)\n",
    "        \n",
    "        # Map pandas types to Athena types\n",
    "        type_mapping = {\n",
    "            'object': 'string',\n",
    "            'int64': 'int',\n",
    "            'float64': 'double',\n",
    "            'bool': 'boolean',\n",
    "            'datetime64[ns]': 'timestamp'\n",
    "        }\n",
    "        \n",
    "        # Generate column definitions\n",
    "        columns = []\n",
    "        for col, dtype in df.dtypes.items():\n",
    "            athena_type = type_mapping.get(str(dtype), 'string')\n",
    "            columns.append(f\"`{col}` {athena_type}\")\n",
    "        \n",
    "        # Create DDL statement\n",
    "        column_definitions = ',\\n    '.join(columns)\n",
    "        s3_location = f's3://{parquet_bucket_name}/{database_name}/'\n",
    "        \n",
    "        ddl = f\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_name} (\n",
    "        {column_definitions}\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '{s3_location}';\"\"\"\n",
    "        \n",
    "        print(f\"\\nGenerating table: {database_name}.{table_name}\")\n",
    "        \n",
    "        # Execute DDL\n",
    "        athena_client = boto3.client('athena')\n",
    "        \n",
    "        # Create table\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=ddl,\n",
    "            QueryExecutionContext={\n",
    "                'Database': database_name\n",
    "            },\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': f's3://{results_bucket_name}/athena-results/'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Wait for table creation\n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        while True:\n",
    "            response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "            state = response['QueryExecution']['Status']['State']\n",
    "            if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        \n",
    "        if state == 'SUCCEEDED':\n",
    "            print(f\"Created table: {database_name}.{table_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to create table {database_name}.{table_name}: {state}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table {database_name}.{table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_all_databases_and_tables(results_bucket_name, parquet_bucket_name):\n",
    "    \"\"\"Create all databases and tables from S3 bucket structure\"\"\"\n",
    "    try:\n",
    "        # Get database and table structure from S3\n",
    "        database_tables = list_s3_folders_and_files(parquet_bucket_name)\n",
    "        if not database_tables:\n",
    "            print(\"No databases/tables found in S3\")\n",
    "            return False\n",
    "            \n",
    "        athena_client = boto3.client('athena')\n",
    "        \n",
    "        # Process each database\n",
    "        for database_name, tables in database_tables.items():\n",
    "            print(f\"\\nProcessing database: {database_name}\")\n",
    "            \n",
    "            # Create database\n",
    "            create_database = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
    "            response = athena_client.start_query_execution(\n",
    "                QueryString=create_database,\n",
    "                ResultConfiguration={\n",
    "                    'OutputLocation': f's3://{results_bucket_name}/athena-results/'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Wait for database creation\n",
    "            query_execution_id = response['QueryExecutionId']\n",
    "            while True:\n",
    "                response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "                state = response['QueryExecution']['Status']['State']\n",
    "                if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "            \n",
    "            if state == 'SUCCEEDED':\n",
    "                print(f\"Created database: {database_name}\")\n",
    "                \n",
    "                # Create each table in the database\n",
    "                for table_name in tables:\n",
    "                    generate_and_create_table(\n",
    "                        results_bucket_name,\n",
    "                        parquet_bucket_name,\n",
    "                        database_name,\n",
    "                        table_name\n",
    "                    )\n",
    "            else:\n",
    "                print(f\"Failed to create database {database_name}: {state}\")\n",
    "                continue\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_all_databases_and_tables: {e}\")\n",
    "        return False\n",
    "\n",
    "results_bucket_name = athena_results_bucket\n",
    "parquet_bucket_name = main_bucket\n",
    "\n",
    "success = create_all_databases_and_tables(results_bucket_name, parquet_bucket_name)\n",
    "if success:\n",
    "    print(\"\\nCompleted creating all databases and tables in Athena!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Text2SQL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get boto3 clients for required AWS services\n",
    "sts_client = boto3.client('sts')\n",
    "iam_client = boto3.client('iam')\n",
    "s3_client = boto3.client('s3')\n",
    "lambda_client = boto3.client('lambda')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('us-west-2', '761018876800')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region, account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random prefix for unique IAM roles, agent name and S3 Bucket and \n",
    "# assign variables\n",
    "suffix = f\"{region}-{account_id}\"\n",
    "agent_name = \"sample-text2sql-agent\"\n",
    "agent_alias_name = \"sample-alias\"\n",
    "bucket_name = f'{agent_name}-{suffix}'\n",
    "bucket_key = f'{agent_name}-schema.json'\n",
    "schema_name = 'sample_text2sql_agent_openapi_schema.json'\n",
    "schema_arn = f'arn:aws:s3:::{bucket_name}/{bucket_key}'\n",
    "bedrock_agent_bedrock_allow_policy_name = f\"{agent_name}-allow-{suffix}\"\n",
    "bedrock_agent_s3_allow_policy_name = f\"{agent_name}-s3-allow-{suffix}\"\n",
    "lambda_role_name = f'{agent_name}-lambda-role-{suffix}'\n",
    "agent_role_name = f'AmazonBedrockExecutionRoleForAgents_{suffix}'\n",
    "lambda_code_path = \"lambda_function.py\"\n",
    "lambda_name = f'{agent_name}-{suffix}'\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 bucket and upload API Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket for Open API schema\n",
    "if region == \"us-east-1\":\n",
    "    s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name\n",
    "    )\n",
    "else:\n",
    "    s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={ 'LocationConstraint': region } \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload Open API Schema to s3 bucket\n",
    "s3_client.upload_file(schema_name, bucket_name, bucket_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Lambda function for Action Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f9e17745-6ef6-4225-8a6b-c211c177acb1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 14 Feb 2025 21:22:07 GMT',\n",
       "   'x-amzn-requestid': 'f9e17745-6ef6-4225-8a6b-c211c177acb1',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create IAM Role for the Lambda function\n",
    "try:\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    assume_role_policy_document_json = json.dumps(assume_role_policy_document)\n",
    "\n",
    "    lambda_iam_role = iam_client.create_role(\n",
    "        RoleName=lambda_role_name,\n",
    "        AssumeRolePolicyDocument=assume_role_policy_document_json\n",
    "    )\n",
    "\n",
    "    # Pause to make sure role is created\n",
    "    time.sleep(10)\n",
    "except:\n",
    "    lambda_iam_role = iam_client.get_role(RoleName=lambda_role_name)\n",
    "\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=lambda_role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package up the lambda function code\n",
    "s = BytesIO()\n",
    "z = zipfile.ZipFile(s, 'w')\n",
    "z.write(lambda_code_path)\n",
    "z.close()\n",
    "zip_content = s.getvalue()\n",
    "\n",
    "# Create Lambda Function\n",
    "lambda_function = lambda_client.create_function(\n",
    "    FunctionName=lambda_name,\n",
    "    Runtime='python3.12',\n",
    "    Timeout=180,\n",
    "    Role=lambda_iam_role['Role']['Arn'],\n",
    "    Code={'ZipFile': zip_content},\n",
    "    Handler='lambda_function.lambda_handler'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IAM policies for agent\n",
    "\n",
    "bedrock_agent_bedrock_allow_policy_statement = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AmazonBedrockAgentBedrockFoundationModelPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"bedrock:InvokeModel\",\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:bedrock:{region}::foundation-model/{model_id}\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "bedrock_policy_json = json.dumps(bedrock_agent_bedrock_allow_policy_statement)\n",
    "\n",
    "agent_bedrock_policy = iam_client.create_policy(\n",
    "    PolicyName=bedrock_agent_bedrock_allow_policy_name,\n",
    "    PolicyDocument=bedrock_policy_json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy that allows fetching of agent's OpenAPI schema from S3\n",
    "\n",
    "bedrock_agent_s3_allow_policy_statement = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowAgentAccessOpenAPISchema\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\"],\n",
    "            \"Resource\": [\n",
    "                schema_arn\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "bedrock_agent_s3_json = json.dumps(bedrock_agent_s3_allow_policy_statement)\n",
    "agent_s3_schema_policy = iam_client.create_policy(\n",
    "    PolicyName=bedrock_agent_s3_allow_policy_name,\n",
    "    Description=f\"Policy to allow invoke Lambda that was provisioned for it.\",\n",
    "    PolicyDocument=bedrock_agent_s3_json\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'da2e77fa-3d49-4299-b255-e78f2f1fee76',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 17 Feb 2025 21:26:54 GMT',\n",
       "   'x-amzn-requestid': 'da2e77fa-3d49-4299-b255-e78f2f1fee76',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create IAM Role for the agent and attach IAM policies\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"bedrock.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "    }]\n",
    "}\n",
    "\n",
    "assume_role_policy_document_json = json.dumps(assume_role_policy_document)\n",
    "agent_role = iam_client.create_role(\n",
    "    RoleName=agent_role_name,\n",
    "    AssumeRolePolicyDocument=assume_role_policy_document_json\n",
    ")\n",
    "\n",
    "# Pause to make sure role is created\n",
    "time.sleep(10)\n",
    "    \n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=agent_role_name,\n",
    "    PolicyArn=agent_bedrock_policy['Policy']['Arn']\n",
    ")\n",
    "\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=agent_role_name,\n",
    "    PolicyArn=agent_s3_schema_policy['Policy']['Arn']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "agent_description = \"Text2SQL agent to run against Bird-SQL Mini-Dev benchmark dataset\"\n",
    "agent_instruction = \"\"\"\n",
    "You are an AI Agent specialized in generating SQL queries for a database. \n",
    "Your primary task is to interpret user queries, \n",
    "generate appropriate SQL queries, and provide relevant answer based on the data. \n",
    "Use only the appropriate tools as required by the specific question. Follow these instructions carefully: \n",
    "1. Before generating any SQL query, use the /getschema tool to familiarize yourself with the database structure. \n",
    "This will ensure your queries are correctly formatted and target the appropriate columns. \n",
    "2. When generating an SQL query: a. Write the query as a single line, removing all newline (\"\n",
    "\") characters. \n",
    "b. Column names should remain consistent, do not modify the column names in the generated SQL query. \n",
    "3. When providing your response: a. Start with a brief summary of your understanding of \n",
    "the user's query. b. Explain the steps you're taking to address the query. c. Ask for clarifications from the \n",
    "user if required.\n",
    "\"\"\"\n",
    "\n",
    "response = bedrock_agent_client.create_agent(\n",
    "    agentName=agent_name,\n",
    "    description=agent_description,\n",
    "    instruction=agent_instruction,\n",
    "    foundationModel=agent_foundation_model,\n",
    "    agentResourceRoleArn=agent_role['Role']['Arn']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AQIMYSEPOK', 'arn:aws:bedrock:us-west-2:761018876800:agent/AQIMYSEPOK')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_id = response['agent']['agentId']\n",
    "agent_arn = response['agent']['agentArn']\n",
    "agent_id , agent_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent Action Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:lambda:us-west-2:761018876800:function:sample-text2sql-agent-us-west-2-761018876800\n",
      "sample-text2sql-agent-us-west-2-761018876800 sample-text2sql-agent-schema.json\n"
     ]
    }
   ],
   "source": [
    "# Pause to make sure agent is created\n",
    "# time.sleep(30)\n",
    "# Now, we can configure and create an action group here:\n",
    "\n",
    "print(lambda_function['FunctionArn'])\n",
    "print(bucket_name, bucket_key)\n",
    "\n",
    "# Use agent helper to clean up\n",
    "agent_action_group_response = bedrock_agent_client.create_agent_action_group(\n",
    "    agentId=agent_id,\n",
    "    agentVersion='DRAFT',\n",
    "    actionGroupExecutor={\n",
    "        'lambda': lambda_function['FunctionArn']\n",
    "        # 'roleArn': 'arn:aws:iam::761018876800:role/sample-text2sql-agent-lambda-role-us-west-2-761018876800',  # Add your IAM role ARN here\n",
    "        # 'roleSessionName': 'BedrockAgentSession'  # Add a session name\n",
    "    },\n",
    "    actionGroupName='QueryAthena',\n",
    "    apiSchema={\n",
    "        's3': {\n",
    "            's3BucketName': bucket_name,\n",
    "            's3ObjectKey': bucket_key\n",
    "        }\n",
    "    },\n",
    "    description='Execute SQL queries in Athena database'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create allow invoke permission on lambda\n",
    "response = lambda_client.add_permission(\n",
    "    FunctionName=lambda_name,\n",
    "    StatementId='allow_bedrock',\n",
    "    Action='lambda:InvokeFunction',\n",
    "    Principal='bedrock.amazonaws.com',\n",
    "    SourceArn=f\"arn:aws:bedrock:{region}:{account_id}:agent/{agent_id}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '194cea38-3947-4aee-ad01-3b86d494664b',\n",
       "  'HTTPStatusCode': 202,\n",
       "  'HTTPHeaders': {'date': 'Mon, 17 Feb 2025 21:53:32 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '119',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '194cea38-3947-4aee-ad01-3b86d494664b',\n",
       "   'x-amz-apigw-id': 'GJhqhGNGvHcEADA=',\n",
       "   'x-amzn-trace-id': 'Root=1-67b3afdc-63233e2f37f3a60e3d0eacee'},\n",
       "  'RetryAttempts': 0},\n",
       " 'agentId': 'AQIMYSEPOK',\n",
       " 'agentStatus': 'PREPARING',\n",
       " 'agentVersion': 'DRAFT',\n",
       " 'preparedAt': datetime.datetime(2025, 2, 17, 21, 53, 32, 819336, tzinfo=tzlocal())}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_prepare = bedrock_agent_client.prepare_agent(agentId=agent_id)\n",
    "agent_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause to make sure agent is prepared\n",
    "# time.sleep(30)\n",
    "agent_alias = bedrock_agent_client.create_agent_alias(\n",
    "    agentId=agent_id,\n",
    "    agentAliasName=agent_alias_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T9J04ATBNJ'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time.sleep(30)\n",
    "# Extract the agentAliasId from the response\n",
    "agent_alias_id = agent_alias['agentAlias']['agentAliasId']\n",
    "\n",
    "agent_alias_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a random id for session initiator id\n",
    "session_id:str = str(uuid.uuid1())\n",
    "\n",
    "# invoke the agent API\n",
    "agentResponse = bedrock_agent_runtime_client.invoke_agent(\n",
    "    inputText=\"what are the open claims?\",\n",
    "    agentId=agent_id,\n",
    "    agentAliasId=agent_alias_id, \n",
    "    sessionId=session_id,\n",
    "    enableTrace=True,\n",
    "    endSession=False,\n",
    "    sessionState={}\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Request sent to Agent:\\n{}\".format(response))\n",
    "print(\"====================\")\n",
    "print(\"Agent processing query now\")\n",
    "print(\"====================\")\n",
    "\n",
    "# Initialize an empty string to store the answer\n",
    "answer = \"\"\n",
    "\n",
    "# Iterate through the event stream\n",
    "for event in response['completion']:\n",
    "    # Check if the event is a 'chunk' event\n",
    "    if 'chunk' in event:\n",
    "        chunk_obj = event['chunk']\n",
    "        if 'bytes' in chunk_obj:\n",
    "            # Decode the bytes and append to the answer\n",
    "            chunk_data = chunk_obj['bytes'].decode('utf-8')\n",
    "            answer += chunk_data\n",
    "\n",
    "# Now 'answer' contains the full response from the agent\n",
    "print(\"Agent Answer: {}\".format(answer))\n",
    "print(\"====================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input .json file for all Text2SQL using ground truth provided by BirdSQL Mini-Dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text2sql-athena-results california_schools\n"
     ]
    }
   ],
   "source": [
    "print(ATHENA_RESULTS_BUCKET_NAME, DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def run_query(query: str, athena_client):\n",
    "    \"\"\"\n",
    "    Run Athena query and return results as pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={\n",
    "                'Database': DATABASE_NAME\n",
    "            },\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': f's3://{ATHENA_RESULTS_BUCKET_NAME}/athena-results/'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        \n",
    "        while True:\n",
    "            response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "            state = response['QueryExecution']['Status']['State']\n",
    "            \n",
    "            if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "                \n",
    "            time.sleep(1)\n",
    "            \n",
    "        if state == 'SUCCEEDED':\n",
    "            response = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "            # Process data\n",
    "            results = []\n",
    "            for row in response['ResultSet']['Rows'][1:]:  # Skip header row\n",
    "                for field in row['Data']:\n",
    "                    value = field.get('VarCharValue', '')\n",
    "                    \n",
    "                    results.append(str(value))\n",
    "            \n",
    "            # Join results with commas\n",
    "            return ', '.join(results)\n",
    "        else:\n",
    "            print(f\"Query failed with state: {state}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error running query: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_ground_truth_answer(question_id: int, question: str, sql_query: str, \n",
    "                               sql_context: str, query_results: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate ground truth answer using AWS Bedrock based on question and query results\n",
    "    \"\"\"\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name='bedrock-runtime'\n",
    "    )\n",
    "        \n",
    "    # Construct prompt for Bedrock\n",
    "    prompt = f\"\"\"You are generating ground truth answers that will be used to evaluate the factual correctness of Text2SQL agent responses.\n",
    "\n",
    "Question: {question}\n",
    "Query Results: {query_results}\n",
    "\n",
    "Generate a natural language answer that:\n",
    "1. States all numerical values and facts from the query results explicitly\n",
    "2. Uses consistent formatting for numbers (maintain exact precision from results)\n",
    "3. Includes all relevant values if multiple results are returned\n",
    "4. States the answer in a clear, declarative way that directly addresses the question\n",
    "5. Avoids additional interpretations or information not present in the query results\n",
    "\n",
    "Remember:\n",
    "- Focus only on the facts present in the query results\n",
    "- Use the exact numbers shown in the results\n",
    "- Structure the answer to make fact-checking straightforward\n",
    "- Be explicit about any percentages, counts, or measurements\n",
    "- Make sure every number in the query results is mentioned in your answer\n",
    "\n",
    "Your answer should be easy to compare with other responses for factual accuracy.\"\"\"\n",
    "\n",
    "    # Create request body for Claude model\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "            }\n",
    "        ],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Call Bedrock\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',  # or your preferred model\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        answer = response_body['content'][0]['text']\n",
    "        \n",
    "        # Format the response in the required structure\n",
    "        formatted_response = {\n",
    "            \"question_id\": question_id,\n",
    "            \"question\": question,\n",
    "            \"question_type\": \"TEXT2SQL\",\n",
    "            \"ground_truth\": {\n",
    "                \"ground_truth_sql_query\": sql_query,\n",
    "                \"ground_truth_sql_context\": sql_context,\n",
    "                \"ground_truth_query_result\": query_results,\n",
    "                \"ground_truth_answer\": answer\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return formatted_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "def get_schema(athena_client):\n",
    "    \"\"\"\n",
    "    Get schema information for all tables in Athena databases\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT\n",
    "            table_name,\n",
    "            column_name,\n",
    "            data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{DATABASE_NAME}'\n",
    "        ORDER BY table_name, ordinal_position;\n",
    "        \"\"\"\n",
    "        \n",
    "    try:\n",
    "        # Start query execution\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=sql,\n",
    "            QueryExecutionContext={\n",
    "                'Database': DATABASE_NAME\n",
    "            }\n",
    "        )\n",
    "            \n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "            \n",
    "        def wait_for_query_completion(query_execution_id):\n",
    "            while True:\n",
    "                response = athena_client.get_query_execution(\n",
    "                    QueryExecutionId=query_execution_id\n",
    "                )\n",
    "                state = response['QueryExecution']['Status']['State']\n",
    "                \n",
    "                if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                    return state\n",
    "                    \n",
    "                time.sleep(2)\n",
    "            \n",
    "        # Wait for query completion\n",
    "        state = wait_for_query_completion(query_execution_id)\n",
    "\n",
    "        if state == 'SUCCEEDED':\n",
    "            # Get query results\n",
    "            results = athena_client.get_query_results(\n",
    "                QueryExecutionId=query_execution_id\n",
    "            )\n",
    "            # Assuming you have a database connection and cursor setup\n",
    "            # cursor.execute(sql)\n",
    "            # results = cursor.fetchall()\n",
    "            \n",
    "            database_structure = []\n",
    "            table_dict = {}\n",
    "\n",
    "            # Skip the header row\n",
    "            rows = results['ResultSet']['Rows'][1:]\n",
    "\n",
    "            for row in rows:\n",
    "                # Extract values from the Data structure\n",
    "                table_name = row['Data'][0]['VarCharValue']\n",
    "                column_name = row['Data'][1]['VarCharValue']\n",
    "                data_type = row['Data'][2]['VarCharValue']\n",
    "                \n",
    "                # Initialize table if not exists\n",
    "                if table_name not in table_dict:\n",
    "                    table_dict[table_name] = []\n",
    "                \n",
    "                # Append column information\n",
    "                table_dict[table_name].append((column_name, data_type))\n",
    "\n",
    "            # Convert to the desired format\n",
    "            for table_name, columns in table_dict.items():\n",
    "                database_structure.append({\n",
    "                    \"table_name\": table_name,\n",
    "                    \"columns\": columns\n",
    "                })\n",
    "\n",
    "            return database_structure\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"Query failed with state: {state}\")\n",
    "    except Exception as e:\n",
    "            print(f\"Error getting schema: {e}\")\n",
    "            raise\n",
    "\n",
    "def generate_dataset(input_file: str, output_file: str, athena_client):\n",
    "    \"\"\"\n",
    "    Generate dataset with ground truth answers in trajectory format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read input file\n",
    "        with open(input_file, 'r') as f:\n",
    "            questions_data = json.load(f)\n",
    "        \n",
    "        # Initialize trajectories dictionary\n",
    "        trajectories = {}\n",
    "        \n",
    "        # Process each question\n",
    "        for idx, item in enumerate(questions_data):\n",
    "            question_id = item.get('question_id', 0)\n",
    "            question = item['question']\n",
    "            sql_query = item['SQL']\n",
    "            \n",
    "            print(f\"\\nProcessing question {question_id}: {question}\")\n",
    "            \n",
    "            # Get table schema\n",
    "            sql_context = get_schema(athena_client)\n",
    "            # Run query\n",
    "            query_results = run_query(sql_query.replace('`','\"'),athena_client)\n",
    "            if query_results is not None:\n",
    "                # Generate answer with formatted response\n",
    "                response = generate_ground_truth_answer(\n",
    "                    question_id=question_id,\n",
    "                    question=question,\n",
    "                    sql_query=sql_query,\n",
    "                    sql_context=str(sql_context),\n",
    "                    query_results=query_results\n",
    "                )\n",
    "                \n",
    "                if response:\n",
    "                    # Create trajectory key\n",
    "                    trajectory_key = f\"Trajectory{idx + 1}\"\n",
    "                    \n",
    "                    # Format the response for this trajectory\n",
    "                    trajectory_response = [response]\n",
    "                    \n",
    "                    # Add to trajectories dictionary\n",
    "                    trajectories[trajectory_key] = trajectory_response\n",
    "                    print(f\"Generated ground truth for question {question_id}\")\n",
    "            \n",
    "        # Write results to output file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(trajectories, f, indent=2)\n",
    "            \n",
    "        print(f\"\\nProcessed {len(trajectories)} questions. Results saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating dataset: {e}\")\n",
    "\n",
    "# TODO: Alter\n",
    "INPUT_FILE = \"questions.json\"\n",
    "OUTPUT_FILE = \"dataset_with_answers.json\"\n",
    "athena_client = boto3.client('athena')\n",
    "\n",
    "generate_dataset(INPUT_FILE, OUTPUT_FILE,athena_client)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
